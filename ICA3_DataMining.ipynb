{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.10"
    },
    "colab": {
      "name": "ICA3_DataMining.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dclark18/7331DataMiningNotebooks/blob/master/ICA3_DataMining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GQYvy9MuPoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1CpzfvrQAfw",
        "colab_type": "code",
        "colab": {},
        "outputId": "c02cd8da-f703-4991-ff96-4755c36b8f87"
      },
      "source": [
        "#  Ebnable HTML/CSS \n",
        "from IPython.core.display import HTML\n",
        "HTML(\"<link href='https://fonts.googleapis.com/css?family=Passion+One' rel='stylesheet' type='text/css'><style>div.attn { font-family: 'Helvetica Neue'; font-size: 30px; line-height: 40px; color: #FFFFFF; text-align: center; margin: 30px 0; border-width: 10px 0; border-style: solid; border-color: #5AAAAA; padding: 30px 0; background-color: #DDDDFF; }hr { border: 0; background-color: #ffffff; border-top: 1px solid black; }hr.major { border-top: 10px solid #5AAA5A; }hr.minor { border: none; background-color: #ffffff; border-top: 5px dotted #CC3333; }div.bubble { width: 65%; padding: 20px; background: #DDDDDD; border-radius: 15px; margin: 0 auto; font-style: italic; color: #f00; }em { color: #AAA; }div.c1{visibility:hidden;margin:0;height:0;}div.note{color:red;}</style>\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<link href='https://fonts.googleapis.com/css?family=Passion+One' rel='stylesheet' type='text/css'><style>div.attn { font-family: 'Helvetica Neue'; font-size: 30px; line-height: 40px; color: #FFFFFF; text-align: center; margin: 30px 0; border-width: 10px 0; border-style: solid; border-color: #5AAAAA; padding: 30px 0; background-color: #DDDDFF; }hr { border: 0; background-color: #ffffff; border-top: 1px solid black; }hr.major { border-top: 10px solid #5AAA5A; }hr.minor { border: none; background-color: #ffffff; border-top: 5px dotted #CC3333; }div.bubble { width: 65%; padding: 20px; background: #DDDDDD; border-radius: 15px; margin: 0 auto; font-style: italic; color: #f00; }em { color: #AAA; }div.c1{visibility:hidden;margin:0;height:0;}div.note{color:red;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2peqRE6QAf3",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "Enter Team Member Names here (double click to edit):\n",
        "\n",
        "- Name 1:Daniel Clark\n",
        "- Name 2:Armando Vela\n",
        "- Name 3:Joe Schueder\n",
        "- Name 4: Jeff Washburn\n",
        "\n",
        "________\n",
        "\n",
        "# In Class Assignment Three\n",
        "In the following assignment you will be asked to fill in python code and derivations for a number of different problems. Please read all instructions carefully and turn in the rendered notebook (or HTML of the rendered notebook)  before the end of class.\n",
        "\n",
        "<a id=\"top\"></a>\n",
        "## Contents\n",
        "* <a href=\"#Loading\">Loading the Data</a>\n",
        "* <a href=\"#distance\">Measuring Distances</a>\n",
        "* <a href=\"#KNN\">K-Nearest Neighbors</a>\n",
        "* <a href=\"#naive\">Naive Bayes</a>\n",
        "\n",
        "________________________________________________________________________________________________________\n",
        "<a id=\"Loading\"></a>\n",
        "<a href=\"#top\">Back to Top</a>\n",
        "## Downloading the Document Data\n",
        "Please run the following code to read in the \"20 newsgroups\" dataset from sklearn's data loading module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbfkO98HQAf4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "c0831ebc-6c65-4bc5-8205-fc6cd551e46f"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups_vectorized\n",
        "import numpy as np\n",
        "from __future__ import print_function\n",
        "\n",
        "# this takes about 30 seconds to compute, read the next section while this downloads\n",
        "ds = fetch_20newsgroups_vectorized(subset='train')\n",
        "\n",
        "# this holds the continuous feature data (which is tfidf)\n",
        "print('features shape:', ds.data.shape) # there are ~11000 instances and ~130k features per instance\n",
        "print('target shape:', ds.target.shape) \n",
        "print('range of target:', np.min(ds.target),np.max(ds.target))\n",
        "print('Data type is', type(ds.data), float(ds.data.nnz)/(ds.data.shape[0]*ds.data.shape[1])*100, '% of the data is non-zero')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "features shape: (11314, 130107)\n",
            "target shape: (11314,)\n",
            "range of target: 0 19\n",
            "Data type is <class 'scipy.sparse.csr.csr_matrix'> 0.121435315436 % of the data is non-zero\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FiZ9vLyQAf7",
        "colab_type": "text"
      },
      "source": [
        "## Understanding the Dataset\n",
        "Look at the description for the 20 newsgroups dataset at http://qwone.com/~jason/20Newsgroups/. You have just downloaded the \"vectorized\" version of the dataset, which means all the words inside the articles have gone through a transformation that binned them into 130 thousand features related to the words in them.  \n",
        "\n",
        "**Question Set 1**:\n",
        "- How many instances are in the dataset? ***11,314 observations. The 130,107 represents all the unique words in the document.***\n",
        "- What does each instance represent? ***Newsgroups corresponding to different topics***\n",
        "- How many classes are in the dataset and what does each class represent? ***130,108 classe***\n",
        "- Would you expect a classifier trained on this data would generalize to documents written in the past week? Why or why not? ***How old is the 20 news groups dataset. Would we generalize the documents written in the past week. Also could be 1995.***\n",
        "- Is the data represented as a sparse or dense matrix? ***Sparse Matrix. each individual word is only used in a handful of documents. Stores infrequently used items in the background and frequent items as their own individual rows.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhVwBMZ5QAf8",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "Enter your answer here:\n",
        "\n",
        "- How many instances are in the dataset? 11314  \n",
        "\n",
        "- What does each instance represent? Documents \n",
        "\n",
        "- How many classes are in the dataset and what does each class represent? The 20 Newsgroups data set. The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents,  partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering. \n",
        "\n",
        "Each subdirectory in the bundle represents a newsgroup; each file in a subdirectory is the text of some newsgroup document that was posted to that newsgroup.   \n",
        "\n",
        "-  How many classes are in the dataset and what does each class represent? Each class represents a newsgroup. There are 20 Classes. alt.atheism comp.graphics comp.os.ms-windows.misc comp.sys.ibm.pc.hardware comp.sys.ibm.pc.hardware comp.sys.mac.hardware comp.windows.x misc.forsale rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey sci.crypt sci.electronics sci.med sci.space soc.religion.christian talk.politics.guns talk.politics.mideast talk.politics.misc talk.religion.misc \n",
        "\n",
        "- Would you expect a classifier trained on this data would generalize to documents written in the past week? Why or why not? The dataset is from 1995, which is 25 years ago. Though many of the high level topics are topics that are discussed today jargon, slang and technology has changed significantly since 1995. I think it may work ok on less technological topics, but not very well at all related to technology.  For example the word corona would have a different meaning than it did in 1995 \n",
        "\n",
        "- Is the data represented as a sparse or dense matrix? The ds command states that it is a sparse matrix. {'data': <11314x130107 sparse matrix of type '<class 'numpy.float64'>' with 1787565 stored elements in Compressed Sparse Row format>,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW3XYiNbQAf9",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "<a id=\"distance\"></a>\n",
        "<a href=\"#top\">Back to Top</a>\n",
        "## Measures of Distance\n",
        "In the following block of code, we isolate three instances from the dataset. The instance \"`a`\" is from the group *computer graphics*, \"`b`\" is from from the group *recreation autos*, and \"`c`\" is from group *recreation motorcycle*. **Exercise for part 2**: Calculate the: \n",
        "- (1) Euclidean distance\n",
        "- (2) Cosine distance \n",
        "- (3) Jaccard similarity \n",
        "\n",
        "between each pair of instances using the imported functions below. Remember that the Jaccard similarity is only for binary valued vectors, so convert vectors to binary using a threshold. \n",
        "\n",
        "**Question for part 2**: Which distance seems more appropriate to use for this data? **Why**?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfT8Dv6NQAf-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "52c90bd8-c0dc-432f-ae2a-ec84e7e1b4a8"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "from scipy.spatial.distance import euclidean\n",
        "from scipy.spatial.distance import jaccard\n",
        "import numpy as np\n",
        "\n",
        "# get first instance (comp)\n",
        "idx = 550\n",
        "a = ds.data[idx].todense()\n",
        "a_class = ds.target_names[ds.target[idx]]\n",
        "print('Instance A is from class', a_class)\n",
        "\n",
        "# get second instance (autos)\n",
        "idx = 4000\n",
        "b = ds.data[idx].todense()\n",
        "b_class = ds.target_names[ds.target[idx]]\n",
        "print('Instance B is from class', b_class)\n",
        "\n",
        "# get third instance (motorcycle)\n",
        "idx = 7000\n",
        "c = ds.data[idx].todense()\n",
        "c_class = ds.target_names[ds.target[idx]]\n",
        "print('Instance C is from class', c_class)\n",
        "\n",
        "# Enter distance comparison below for each pair of vectors:\n",
        "p = 'Placeholder'\n",
        "print('\\n\\nEuclidean Distance\\n ab:', euclidean(a,b), 'ac:', euclidean(a,c), 'bc:',euclidean(b,c))\n",
        "print('Cosine Distance\\n ab:', cosine(a,b), 'ac:', cosine(a,c), 'bc:', cosine(b,c))\n",
        "print('Jaccard Dissimilarity (vectors should be boolean values)\\n ab:', jaccard(a>0,b>0), 'ac:', jaccard(a>0,c>0), 'bc:', jaccard(b>0,c>0))\n",
        "\n",
        "print('The most appropriate distance is Cosine.Because the distances generated from this method make most sense. Bikes and Autos are more related, and Computers are the same distance from Bikes/autos.That is visible from Cosine distance results.')\n",
        "print('Because the distances generated from this method make most sense. Bikes and Autos are more related, and Computers are the same distance from Bikes/autos.That is visible from Cosine distance results.') "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Instance A is from class comp.graphics\n",
            "Instance B is from class rec.autos\n",
            "Instance C is from class rec.motorcycles\n",
            "\n",
            "\n",
            "Euclidean Distance\n",
            " ab: 1.09851846719 ac: 1.18914054254 bc: 0.917779422666\n",
            "Cosine Distance\n",
            " ab: 0.6033714113755322 ac: 0.7070276149559529 bc: 0.4211595343347173\n",
            "Jaccard Dissimilarity (vectors should be boolean values)\n",
            " ab: 0.8821138211382114 ac: 0.8754716981132076 bc: 0.9087947882736156\n",
            "The most appropriate distance is Cosine.Because the distances generated from this method make most sense. Bikes and Autos are more related, and Computers are the same distance from Bikes/autos.That is visible from Cosine distance results.\n",
            "Because the distances generated from this method make most sense. Bikes and Autos are more related, and Computers are the same distance from Bikes/autos.That is visible from Cosine distance results.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxYoipOOQAgB",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "# Start of Live Session Assignment\n",
        "___\n",
        "<a id=\"KNN\"></a>\n",
        "<a href=\"#top\">Back to Top</a>\n",
        "## Using scikit-learn with KNN\n",
        "Now let's use stratified cross validation with a holdout set to train a KNN model in `scikit-learn`. Use the example below to train a KNN classifier. The documentation for `KNeighborsClassifier` is here: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html  \n",
        "\n",
        "**Exercise for part 3**: Use the code below to test what value of `n_neighbors` works best for the given data. *Note: do NOT change the metric to be anything other than `'euclidean'`. Other distance functions are not optimized for the amount of data we are working with.* \n",
        "\n",
        "**Question for part 3**: What is the accuracy of the best classifier you can create for this data (by changing only the `n_neighbors` parameter)? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qTSBlPqQAgB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "59a87a80-aee5-4c32-e8e6-17bf792a6a89"
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from IPython.html import widgets \n",
        "\n",
        "cv = StratifiedShuffleSplit(test_size = 0.5, train_size=0.5, random_state=12345)\n",
        "\n",
        "# fill in the training and testing data and save as separate variables\n",
        "for trainidx, testidx in cv.split(ds.data,ds.target):\n",
        "    # note that these are sparse matrices\n",
        "    X_train = ds.data[trainidx] \n",
        "    X_test = ds.data[testidx] \n",
        "    y_train = ds.target[trainidx]\n",
        "    y_test = ds.target[testidx]\n",
        "\n",
        "# fill in your code  here to train and test\n",
        "# calculate the accuracy and print it for various values of K\n",
        "acc_dict = {}\n",
        "for K in range(1, 11):\n",
        "  clf = KNeighborsClassifier(n_neighbors=K, weights='uniform', metric='euclidean')\n",
        "  clf.fit(X_train, y_train)\n",
        "  yhat = clf.predict(X_test)\n",
        "  print('Accuracy of classifier with %d neighbors is: %.2f'%(K,accuracy_score(y_test, yhat)))\n",
        "\n",
        "  acc_dict.update( {K : accuracy_score(y_test, yhat)} )\n",
        "\n",
        "\n",
        "maximum = max(acc_dict, key=acc_dict.get)\n",
        "acc = acc_dict[maximum] # overwrite this with the actual accuracy\n",
        "\n",
        "#=====================================\n",
        "\n",
        "print('The maximum accuracy of the classifier is %.2f with %d neighbors'%(acc,maximum))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of classifier with 1 neighbors is: 0.60\n",
            "Accuracy of classifier with 2 neighbors is: 0.56\n",
            "Accuracy of classifier with 3 neighbors is: 0.53\n",
            "Accuracy of classifier with 4 neighbors is: 0.51\n",
            "Accuracy of classifier with 5 neighbors is: 0.49\n",
            "Accuracy of classifier with 6 neighbors is: 0.48\n",
            "Accuracy of classifier with 7 neighbors is: 0.47\n",
            "Accuracy of classifier with 8 neighbors is: 0.47\n",
            "Accuracy of classifier with 9 neighbors is: 0.46\n",
            "Accuracy of classifier with 10 neighbors is: 0.45\n",
            "The maximum accuracy of the classifier is 0.60 with 1 neighbors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtsKoA2mQAgE",
        "colab_type": "text"
      },
      "source": [
        "The best accuracy is 0.60 with 1 neighbor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z6hz_ClQAgF",
        "colab_type": "text"
      },
      "source": [
        "**Question for part 3**: With sparse data, does the use of a KDTree representation make sense? Why or Why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q0kDSoJQAgG",
        "colab_type": "text"
      },
      "source": [
        "No. The use of a KD Tree does not make sense when you are working with Sparse Data.  The cost of Creating a Tree is going to be much greater than that of building a tree using KNN when there is text involved and the number of classifiers. With KNN, measuring distance is relatively easy on a computer, while building a tree will be difficult to branch since it's highly binary and is going to have very many zeros. \n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2yEpPmsBQAgH",
        "colab_type": "text"
      },
      "source": [
        "_____\n",
        "## KNN extensions - Centroids\n",
        "Now lets look at a very closely related classifier to KNN, called nearest centroid. In this classifier (which is more appropriate for big data scenarios and sparse data), the training step is used to calculate the centroids for each class. These centroids are saved. Unknown attributes, at prediction time, only need to have distances calculated for each saved centroid, drastically decreasing the time required for a prediction. \n",
        "\n",
        "**Exercise for part 4**: Use the template code below to create a nearest centroid classifier. Test which metric has the best cross validated performance: Euclidean, Cosine, or Manhattan. In `scikit-learn` you can see the documentation for NearestCentroid here: \n",
        "- http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid\n",
        "\n",
        "and for supported distance metrics here:\n",
        "- http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.distance_metrics.html#sklearn.metrics.pairwise.distance_metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq1m4wESQAgI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "05d3883d-692b-4cd6-ee29-5927d5ad30e5"
      },
      "source": [
        "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
        "\n",
        "# the parameters for the nearest centroid metric to test are:\n",
        "#    l1, l2, and cosine (all are optimized)\n",
        "#•\tUse a for loop to test your different distance metrics\n",
        "#\tfor d in ['l1','l2','cosine']:\n",
        "#\tclf = NearestCentroid(metric=d)  \n",
        "for distance in ['l1', 'cosine', 'l2']:\n",
        "  clf = NearestCentroid(metric=distance)\n",
        "  clf.fit(X_train, y_train)\n",
        "  yhat = clf.predict(X_test)\n",
        "  acc = accuracy_score(y_test, yhat)\n",
        "\n",
        "  print(distance, acc)\n",
        "\n",
        "# fill in your code here\n",
        "\n",
        "\n",
        "print('The best distance metric is: Cosine since it is the best accuracy')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l1 0.30245713275587766\n",
            "cosine 0.4811737670143185\n",
            "l2 0.4132932649814389\n",
            "The best distance metric is: Cosine since it is the best accuracy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuviyaBrQAgN",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "<a id=\"naive\"></a>\n",
        "<a href=\"#top\">Back to Top</a>\n",
        "## Naive Bayes Classification\n",
        "Now let's look at the use of the Naive Bayes classifier. The 20 newsgroups dataset has 20 classes and about 130,000 features per instance. Recall that the Naive Bayes classifer calculates a posterior distribution for each possible class. Each posterior distribution is a multiplication of many conditional distributions: \n",
        "\n",
        "$${\\arg \\max}_{j} \\left(p(class=j)\\prod_{i} p(attribute=i|class=j) \\right)$$\n",
        "\n",
        "where $p(class=j)$ is the prior and $p(attribute=i|class=j)$ is the conditional probability.\n",
        "\n",
        "**Question for part 5**: With this many classes and features, how many different conditional probabilities need to be parameterized? How many priors need to be parameterized?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLW9-GGehTDa",
        "colab_type": "text"
      },
      "source": [
        "Since we have 130,107 features along with 20 classes, we will need to multipy the two to get 2,602,140 conditional probabilities. There is one prior for each class, to make a total of 20 priors which must be parameterized. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YMLgLi7QAgR",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "## Naive Bayes in Scikit-learn\n",
        "Scikit has several implementations of the Naive Bayes classifier: `GaussianNB`, `MultinomialNB`, and `BernoulliNB`. Look at the documentation here: http://scikit-learn.org/stable/modules/naive_bayes.html Take a look at each implementation and then answer this question: \n",
        "\n",
        "**Questions for part 6**: \n",
        "- If the instances contain mostly continuous attributes, would it be better to use Gaussian Naive Bayes, Multinomial Naive Bayes, or Bernoulli? And Why? \n",
        "- What if the data is sparse, does this change your answer? Why or Why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUGkKfysQAgS",
        "colab_type": "text"
      },
      "source": [
        "Enter you answer here:\n",
        "\n",
        "1. ) For continuous attributes, Gaussian Naive Bayes as it normalizes continuous data to fit a naive Bayes format, which minimizes the effect of dependence. That said, it would assume that the data is normally distributed on each feature. \n",
        "Multinomial and Binaries are better suited for binary and nominal/ordinal classes, and not appropriate for continuous data. \n",
        "\n",
        "2.) In dealing with sparse data, we would not want to use a Gaussian Naive Bayes, and would rather use Multinomial Naive Bayes instead, as it's better suited for text data. Sparse matrices are very difficult to capture realistic Gaussian models since the mean is often close to zero. \n",
        "\n",
        "You could also potentially binarize our text classification data for a specific model, and in that case a Bernoulli Naive Bayes will work. \n",
        "\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5o9jBneQAgT",
        "colab_type": "text"
      },
      "source": [
        "## Naive Bayes Comparison\n",
        "For the final section of this notebook let's compare the performance of Naive Bayes for document classification. Look at the parameters for `MultinomialNB`, and `BernoulliNB` (especially `alpha` and `binarize`). \n",
        "\n",
        "**Exercise for part 7**: Using the example code below, change the parameters for each classifier and see how accurate you can make the classifiers on the test set. \n",
        "\n",
        "**Question for part 7**: Why are these implementations so fast to train? What does the `'alpha'` value control in these models (*i.e.*, how does it change the parameterizations)? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtPnryCZQAgU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "f58d0e78-760b-4cf2-fe08-8b018306f346"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "a_list = [0.0, 0.001, 0.01, 0.1, 1]\n",
        "b_list = [0.0, 0.002, 0.02, 0.04, 0.06, 0.08, 0.1]\n",
        "\n",
        "for a in a_list:\n",
        "    clf_mnb = MultinomialNB(alpha= a)\n",
        "    acc = clf_mnb.fit(X_train, y_train).score(X_test, y_test)\n",
        "    print('Accuracy of MultinomialNB classifier for alpha = %.3f is: %.3f'%(a,acc))\n",
        "    \n",
        "\n",
        "for a in a_list:\n",
        "    print ('='*80)\n",
        "    for b in b_list:\n",
        "        clf_bnb = BernoulliNB(alpha=a, binarize=b)\n",
        "        acc = clf_bnb.fit(X_train, y_train).score(X_test, y_test)\n",
        "        print(\"Accuracy of BernoulliNB classifier for (alpha=%.3f, binarize=%.3f) is: %.3f\" % (a, b, acc))\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of MultinomialNB classifier for alpha = 0.000 is: 0.869\n",
            "Accuracy of MultinomialNB classifier for alpha = 0.001 is: 0.893\n",
            "Accuracy of MultinomialNB classifier for alpha = 0.010 is: 0.887\n",
            "Accuracy of MultinomialNB classifier for alpha = 0.100 is: 0.833\n",
            "Accuracy of MultinomialNB classifier for alpha = 1.000 is: 0.718\n",
            "================================================================================\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.000, binarize=0.000) is: 0.856\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.000, binarize=0.002) is: 0.857\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.000, binarize=0.020) is: 0.870\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.000, binarize=0.040) is: 0.847\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.000, binarize=0.060) is: 0.791\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.000, binarize=0.080) is: 0.741\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.000, binarize=0.100) is: 0.680\n",
            "================================================================================\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.001, binarize=0.000) is: 0.841\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.001, binarize=0.002) is: 0.845\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.001, binarize=0.020) is: 0.876\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.001, binarize=0.040) is: 0.862\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.001, binarize=0.060) is: 0.808\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.001, binarize=0.080) is: 0.758\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.001, binarize=0.100) is: 0.690\n",
            "================================================================================\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.010, binarize=0.000) is: 0.823\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.010, binarize=0.002) is: 0.830\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.010, binarize=0.020) is: 0.872\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.010, binarize=0.040) is: 0.864\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.010, binarize=0.060) is: 0.812\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.010, binarize=0.080) is: 0.756\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.010, binarize=0.100) is: 0.696\n",
            "================================================================================\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.100, binarize=0.000) is: 0.786\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.100, binarize=0.002) is: 0.794\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.100, binarize=0.020) is: 0.850\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.100, binarize=0.040) is: 0.844\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.100, binarize=0.060) is: 0.777\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.100, binarize=0.080) is: 0.708\n",
            "Accuracy of BernoulliNB classifier for (alpha=0.100, binarize=0.100) is: 0.639\n",
            "================================================================================\n",
            "Accuracy of BernoulliNB classifier for (alpha=1.000, binarize=0.000) is: 0.633\n",
            "Accuracy of BernoulliNB classifier for (alpha=1.000, binarize=0.002) is: 0.637\n",
            "Accuracy of BernoulliNB classifier for (alpha=1.000, binarize=0.020) is: 0.713\n",
            "Accuracy of BernoulliNB classifier for (alpha=1.000, binarize=0.040) is: 0.666\n",
            "Accuracy of BernoulliNB classifier for (alpha=1.000, binarize=0.060) is: 0.525\n",
            "Accuracy of BernoulliNB classifier for (alpha=1.000, binarize=0.080) is: 0.431\n",
            "Accuracy of BernoulliNB classifier for (alpha=1.000, binarize=0.100) is: 0.364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCd21ESnpgxj",
        "colab_type": "text"
      },
      "source": [
        "Within the Multinomial classifier, the top alpha score we found was alpha 0.001, which gave us an accuracy of 0.893. \n",
        "\n",
        "Within Bernouli's Classifier, the top binarize score was 0.020 across all the alpha selections we chose. In addition, as long as our alpha is below, 0.1, we can get a high accuracy. \n",
        "\n",
        "The reason they are able to perform so quickly is that the model is built on counting and dividing to create probabilities. In addition, naive bayes assumes independence which decouples the features from one another without the need to account for many weights. Also there is no iteration, there is no epoch, and the alpha values helps to smoothen the model and reduce the impact of features that don't play a role in model performance, and reduces computation required. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "0URVN3_ZQAgW",
        "colab_type": "text"
      },
      "source": [
        "________________________________________________________________________________________________________\n",
        "\n",
        "That's all! Please **upload your rendered notebook to blackboard** and please include **team member names** in the notebook submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y54ZWcUQAgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}